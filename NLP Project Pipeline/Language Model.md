# Language Model 语言模型
[TOC]
## 前言

本小节内容主要介绍语言模型的相关知识。

## 1 Noise Channel Model 噪声信道模型

是一个普适性的建模模型，被广泛应用于语音识别、拼写纠错、机器翻译、中文分词、词性标注、音字转换等众多领域。

用公式来表示就是，
$$
p(text|source)\propto p(source|text)p(text)
$$
$p(text)$这部分的概率就是通过语言模型得到的。

例如，机器翻译
$$
p(中文|英文)\propto p(英文|中文)p(中文)
$$
其中，p(中文)由语言模型得到。

## 2 语言模型

用来判断一句话是否在语法上通顺。语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为$T$的文本中的词依次为$w_1,w_2,...,w_T$，那么在离散的时间序列中，$w_t（1≤t≤T）$可看作在时间步（time step）$t$的输出或标签。给定一个长度为$T$的词的序列$w_1,w_2,...,w_T$，语言模型将计算该序列的概率：
$$
p(w_1,w_2,...,w_T)
$$
语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。

### 2.1 Chain Rule 链式法则

链式法则如下公式所示
$$
p(A,B,C,D)=p(A)p(B|A)p(C|A,B)p(D|A,B,C)
$$
上面公式的前提是不认为$A,B,C,D$是相互独立的。

通过具体例子来说明，
$$
p(厨房，里面，食用油，用光，了)=p(厨房)p(里面|厨房)p(食用油|厨房，里面)p(用光|厨房，里面，食用油)p(了|厨房，里面，食用油，用光，了)
$$
实际计算的过程中，通过统计的方法来计算概率，通过计数的方法计算概率。但是会遇到稀疏性的问题，即存在某些组合在我们的预料中不存在的情况，导致概率越来越低，甚至为零。

### 2.2 马尔可夫假设（Markov Assumption）与N元文法（N-Gram）

> 当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order nn）。

当n分别等于1，2，3时，分别将其称作一元语法（unigram），二元语法（bigram）和三元语法（trigram）。

通过实例来解释这几种语法，

* 一元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2)...p(w_t)
  $$

* 二元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2|w_1)p(w_3|w_2)...p(w_t|w_{t-1})
  $$

* 三元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)...p(w_t|w_{t-1},w_{t-2})
  $$

## 3 基于语言模型计算文本的概率

根据不同的文法规定，采用不同的计算公式计算出对应的概率，统计的方法来计算概率就是通过计数的方式，上文提到了，如果有些单词或者单词序列在语料中不存在，那么概率会变成零，需要采用一些平滑策略来解决该问题。

![](D:\yinbo.qiao\Study Folder\github\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-15_16-50-51.jpg)

## 4 评估语言模型

问题：如何评估训练出来的两个语言模型的好坏？

前提：

* 假设有两个语言模型A，B
* 选定一个特定的任务比如拼写纠错
* 把两个模型都应用于该任务中
* 最后比较准确率，从而判断A，B的表现

但是上面的方法效率太低，时间耗费太长，还要指定相应的下游任务，这样的方法并不理想。于是我们找到另外一种方式来进行评估。

核心思路就是，将训练好的语言模型做一个生成任务，比如完形填空的方式，给出一个单词，来生成下一个单词。

今天____

今天天气____

......

采用的评估准则就是Perplexity困惑度。
$$
Perplexity = 2^{-(x)}\ \ \ \ x:average log likelihood
$$
通过一个例子来说明，如何计算困惑度，

![](D:\yinbo.qiao\Study Folder\github\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-15_20-25-29.jpg)

一般情况下，Trigram的训练效果是最好的，如果n更大的话，可能会过拟合，另一方面也会提升算法的时间复杂度。

## 5 语言模型的平滑方法

我们观察第三小节中的语料库，如果当前有两句话，

> 句子1：我今天健身没有
>
> 句子2：我今天没有健身

由于语料库中不存在`没有`这个单词，于是两句话计算出来的概率都是0，但是从人类的感知，明显第一个句子要更加的符合语法，也更流畅，但是由于语料库的限制导致了无法区分两个句子的合理性，为了解决这一点，使用平滑方法（Smoothing）。

### 5.1 Add-one smoothing（Laplace Smoothing）加一平滑/拉普拉斯平滑

单词计数加一，防止概率为0。
$$
P_{Add-one}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)+\color{red}{1}}{c(w_i)+\color{red}{V}}
$$
加$V$是为了保证，最终所有概率加和等于1。

### 5.2 Add-K Smoothing

$$
P_{Add-one}(w_i|w_{i-1})=\frac{c(w_{i-1},w_i)+\color{red}{k}}{c(w_i)+\color{red}{kV}}
$$

这里面的k可以通过语言模型计算出来，
$$
\hat k = argmax_{k}{f(k)}
$$
将训练好的语言模型放到验证集上进行验证，最小化上述公式的输入，得到$\hat k$。

### 5.3 Interpolation 插值

基于训练数据中得到的数值，用来计算其他概率。如下所示，

C(in the kictchen) = 0

C(the kitchen) = 3

C(kitchen) = 4

C(arboretum) = 0

------

通过上面的四个计数数值，计算下面两个概率，同时都为零。

p(kitchen|in the) = 0

p(arboretum|in the) = 0

显然，为了公平的比较这两个概率，因为kitchen出现的概率更高，因此倾向于认为`in the kitchen'的概率更高。
$$
p(w_n|w_{n-1},w_{n-2})=\lambda_1p(w_m|w_{n-1},w_{n-2})+\lambda_2p(w_n|w_{n-1})+\lambda_3p(w_n)
$$

### 5.4 Good-Turning Smoothing

![](D:\Document\Nutscloud\我的坚果云\git repo\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-17_11-40-01.jpg)

将这种现象应用到语言模型的平滑中，得到了goog-turning smoothing的算法。

**步骤：**

1. 统计词频 $N_c$ 出现c词的单词个数

   Sam I am I am Sam I do not eat.

   | Word 次数 | Nc   |
   | --------- | ---- |
   | Sam 2     | N1=3 |
   | I 3       | N2=2 |
   | am 2      | N3=1 |
   | do 1      |      |
   | not 1     |      |
   | eat 1     |      |

2. 算法

   * 没有出现的单词如何处理
     $$
     P_{MLE}= 0\\
     P_{GT}=\frac{N_1}{N}
     $$
     MLE代表极大似然估计，没出现过就认为是0，GT代表good-turning，没出现过的使用N1来估计。

   * 因为将一部分概率分给了未知单词，单词原来出现过的单词概率调整为
     $$
     P_{MLE}=\frac{c}{N}\\
     P_{GT} = \frac{(c+1)N_{c+1}}{N_cN}
     $$
     GT代表good-turning算法。

虽然Good-Turning算法是一个非常漂亮的算法，但是有一个致命的缺点。

**缺点：**

Nc的统计量不能保证一定存在，比如语料中最多出现23个重复单词，但是估计23需要24的单词个数，但是却没有该怎么办？另一种现象就是，有出现101次的单词数目，但是没有102，103次出现的单词数目，此时该怎么办呢？

**解决方法**：

通过插值算法，或者机器学习算法来拟合词频的曲线，估计出缺失值的大概大小，从而解决上述问题。

## 6 利用语言模型生成句子

![](D:\Document\Nutscloud\我的坚果云\git repo\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-17_12-11-14.jpg)

## 总结

本小节，介绍了语言模型的相关知识，并不全面只是一些基础知识，也没有涉及当下非常火热的预训练语言模型。