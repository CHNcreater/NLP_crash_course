# 文本表示 Word Representation
[TOC]
## 前言

经过分词和清洗之后的文本，要想利用他们，则需要将他们转化为文本向量表示，才能输入模型中进行计算，生成文本向量表示的方法有很多，其理论基础可以参考我的这篇[知乎文章](https://zhuanlan.zhihu.com/p/344474638)。

## 1 文本表示

**One-hot**

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic4.zhimg.com%2Fv2-289a49efd5a3687c6bb3dfad077fbb53_b.jpg&refer=http%3A%2F%2Fpic4.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1626144917&t=3b304284a1c2dce0a468614ebdce40e9)

*注：图片来自@知乎华来知识*

**句子表示（布尔）**

词典： ["我","你","他","晚上","想","睡觉","吃","烤鸭"]

句子1：我想吃烤鸭     -->      [1, 0, 0, 0, 1, 0, 1, 1]

句子2：他想睡觉         -->      [0, 0, 1, 0, 1, 1, 0, 0]

**计数句子向量 count-based representation**

词典： ["我","你","他","晚上","想","睡觉","吃","烤鸭"]

句子1：想吃烤鸭想睡觉     -->      [0，0，0，0，2，1，1，1]

## 2 文本相似度

### 2.1 通过欧氏距离计算句子相似度

$$
d= ||s_1-s_2||_2
$$

距离越小，相似度越大，距离越大，相似度越小。

### 2.2 余弦相似度计算句子相似度

在实际中，是使用最多的计算相似度的方法。
$$
d = \frac{s_1\cdot s_2}{||s_1||_2\cdot||s_2||_2}
$$

余弦相似度越大，句子越相似，反之，句子越不相似。

第一小节中介绍的三种向量表示方式，存在很大的问题，不能突出句子中重要单词的重要性，有时这中重要性可能会被弄反，为了避免这种问题，提出了tf-idf的文本表示方法。

## 3 Tf-idf文本表示

公式介绍
$$
tfidf(w) = tf(d,w)*idf(w)
$$
$tf(d,w)$代表文档d中w的词频，$idf(w)=log\frac{N}{N(w)}$，N表示语料库中的文档总数，N(w)表示词语w出现在多少个文档中。

> TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。**

**一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.**

通过一个例子来加深tf-idf的理解

![](.\Word Representation.assets\2021-06-13_11-31-16.jpg)

上述提到的几种向量表示方式，其实都属于one-hot向量，思想都是差不多的。但是上述这几种向量表示方式，并不能很好的获取单词之间的语义关联，比如“健身”和“强壮”的关联性并不能通过one-hot向量很好的体现出来。

尤其是第一张图片中的各种动物向量表示之间的欧氏距离都是根号2，都是相同的值，根本体现不出来某两个动物之间的相似度关系。

第二个问题是独热向量的表示，太过于稀疏。

总结起来，独热向量有两点缺点：

1. 无法体现语义相似度关系。
2. 向量过于稀疏。

因此，我们得出结论，我们不可能通过one-hot representation来表达语义相似度。从而，提出了词向量的概念。

## 4 词向量

这里的词向量，其实就是分布式表示方法，解决了上面独热向量的两个问题。

![](.\Word Representation.assets\one-hot-distribution representation.png)

100维的独热向量最多可以表大100个单词，而100维的分布式表示法，理论上可以容纳正无穷个不同单词。

使用深度学习模型来训练词向量，这里有几个经典的模型，比如skip-gram，Glove，CBOW，RNN/LSTM，MF（matrix factorization），Gaussian Embedding等。

<font color="red">很多时候对我们来说，训练词向量是费时费力的，通常我们直接使用大公司训练好的词向量，但是与如遇到一些垂直领域，比如金融，医药领域，则需要我们自己训练一个领域适应的词向量。</font>

![](.\Word Representation.assets\distribution representation.png)

这里我们认为，每个词向量代表了该单词的语义，代表该单词的意思。

**如何根据词向量区表大文章/句子向量？**

平均的方法

将每个词向量加和起来求平均。

我们 = [0.1，0.2，0.1，0.3]

去=[0.3，0.2，0.15，0.2]

运动=[0.2，0.15，0.4，0.7]

sum=[0.6，0.55，0.65，1.2]

ave=[0.2，0.18，0.22，0.4]

我们去运动=[0.2，0.18，0.22，0.4]

## 5 倒排表

在面对基于检索的问答系统时，

![](.\Word Representation.assets\2021-06-14_11-54-23.jpg)

倒排表是建立单词到文本的映射关系，以下面的句子为例，介绍一下倒排索引表。

| 文本号 | 文本              |
| ------ | ----------------- |
| Doc1   | 今天/天气/不错    |
| Doc2   | 明天/天气/也/不错 |
| Doc3   | 今天/去/运动      |

词典：[今天，明天，天气，不错，也，运动，去]

倒排索引表

| 单词 | 索引       |
| ---- | ---------- |
| 今天 | Doc1，Doc3 |
| 明天 | Doc2       |
| 天气 | Doc1，Doc2 |
| 不错 | Doc1，Doc2 |
| 也   | Doc2       |
| 运动 | Doc3       |
| 去   | Doc3       |

这样，根据输入的句子的分词结果，找到单词所在的文档，这里可以采用交集，或者并集的方式，提供给下一步，计算相似度等操作，得到最终答案。

## 总结

本讲介绍了文本表示相关的内容，涉及到one-hot词向量表示和分布式词向量表示，以及倒排索引的相关知识。