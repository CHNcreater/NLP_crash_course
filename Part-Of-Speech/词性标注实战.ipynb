{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS part-of speech 词性标注\n",
    "## 任务描述\n",
    "给定一个字符序列，标注出每个字符的词性\n",
    "$$S = w_1,w_2.w_3,w_4,w_5$$\n",
    "$$Z=z_1,z_2,z_3,z_4,z_5$$\n",
    "## 解决方法\n",
    "现根据标注好的词性表进行训练，之后根据输入的字符序列，标注出各个字符对应的词性\n",
    "$$S'=w_1',w_2',w_3',w_4',w_5'$$\n",
    "$$Z'=?$$\n",
    "## 理论基础\n",
    "**Noisy Channel Model**\n",
    "$$\\begin{align}P(Z|S) &\\propto P(S|Z)P(Z)\\\\\n",
    "&=P(w_1w_2w_3...w_n|z_1z_2z_3...z_n)P(z_1z_2z_3...z_n)\\\\\n",
    "&=\\prod_{i=1}^nP(w_i|z_i)\\cdot P(z_1)P(z_2|z_1)...P(z_n|z_{n-1})\n",
    "\\end{align}$$\n",
    "其中$\\prod_{i=1}^nP(w_i|z_i)$被称为是条件概率，$P(z_1)P(z_2|z_1)...P(z_n|z_{n-1}$是语言模型。\n",
    "$$\\begin{align}\n",
    "\\hat Z &= argmaxP(Z|S)\\\\\n",
    "&=argmax\\prod_{i=1}^nP(w_i|z_i)\\cdot P(z_1)\\prod_{j=2}^nP(z_j|z_{j-1})\\\\\n",
    "&\\propto log(\\prod_{i=1}^nP(w_i|z_i)\\cdot P(z_1)\\prod_{j=2}^nP(z_j|z_{j-1}))\\\\\n",
    "&=argmax\\sum_{i=1}^nlog(w_i|z_i)+logP(z_1)+\\sum_{j=2}^nlogP(z_j|z_{j-1})\n",
    "\\end{align}$$\n",
    "为了求出$\\hat Z$则需要计算上述公式，我们需要计算$\\sum_{i=1}^nlog(w_i|z_i)$，$logP(z_1)$，$\\sum_{j=2}^nlogP(z_j|z_{j-1})$，分别记为$A, \\pi, B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id, id2tag = {}, {}# 词性字典，词性转id，id转词性\n",
    "word2id, id2word = {}, {}# 单词id字典，id单词字典\n",
    "for line in open('traindata.txt','r',encoding='utf-8'):# 读取文件，生成相关字典\n",
    "    items = line.split('/')\n",
    "    word, tag = items[0],items[1].strip()\n",
    "    \n",
    "    if word not in word2id:\n",
    "        word2id[word] = len(word2id)\n",
    "        id2word[len(id2word)] = word\n",
    "    if tag not in tag2id:\n",
    "        tag2id[tag] = len(tag2id)\n",
    "        id2tag[len(id2tag)] = tag\n",
    "M = len(word2id) # M:词典的大小，# of words in dictionary\n",
    "N = len(tag2id) # N：词性的种类个数，# of tags in tag dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18978 54\n"
     ]
    }
   ],
   "source": [
    "print(M,N)\n",
    "# print(word2id, id2word)\n",
    "# print(tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建$A, B, \\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi = np.zeros(N) # 初始化pi向量，表示出现在第一个位置的各个词性的概率\n",
    "A = np.zeros((N,M)) # A[i][j]表示在第i个词性下单词是第j个的概率 ，发射矩阵\n",
    "B = np.zeros((N,N)) # B[i][j]表示从第j个词性转移到第i个词性的概率，转移矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据训练数据，统计相关的数据，然后标准化，得到相应的A, B, pi\n",
    "prev_tag = \"\"\n",
    "for line in open(\"traindata.txt\",\"r\",encoding=\"utf-8\"):\n",
    "    items = line.split(\"/\")\n",
    "    wordID, tagID = word2id[items[0]],tag2id[items[1].strip()]\n",
    "    if prev_tag == \"\":\n",
    "        pi[tagID] += 1\n",
    "        A[tagID][wordID] += 1\n",
    "        prev_tag = tagID\n",
    "    else:\n",
    "        A[tagID][wordID] += 1\n",
    "        B[tag2id[prev_tag]][tagID] += 1\n",
    "    if items[0] == \".\":\n",
    "        prev_tag = \"\"\n",
    "    else:\n",
    "        prev_tag = items[1].strip()\n",
    "# normalization\n",
    "pi = pi/sum(pi)\n",
    "for i in range(N):\n",
    "    A[i] /= sum(A[i])\n",
    "    B[i] /= sum(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.67741935e-04 0.00000000e+00 0.00000000e+00 ... 5.16129032e-05\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "[1.81425219e-01 0.00000000e+00 1.00037051e-02 3.33456836e-03\n",
      " 3.95208102e-03 3.68037545e-02 1.11646289e-01 3.66802519e-02\n",
      " 6.17512659e-04 3.81622823e-02 8.76867976e-03 5.18710634e-02\n",
      " 6.02692355e-02 2.47005064e-04 2.17240953e-01 0.00000000e+00\n",
      " 1.48203038e-03 6.05162406e-03 8.64517723e-04 2.47005064e-04\n",
      " 0.00000000e+00 4.73014697e-02 0.00000000e+00 7.16314684e-03\n",
      " 1.72903545e-03 2.09954304e-03 7.53365444e-02 6.36038039e-02\n",
      " 2.59355317e-03 1.85253798e-03 5.92812153e-03 1.97604051e-03\n",
      " 2.84055823e-03 0.00000000e+00 0.00000000e+00 2.71705570e-03\n",
      " 5.92812153e-03 5.92812153e-03 9.88020254e-04 3.70507595e-04\n",
      " 1.23502532e-04 0.00000000e+00 0.00000000e+00 1.85253798e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(A[0])\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(v):# 计算log函数值\n",
    "    if v == 0:\n",
    "        return np.log(v+0.0001)\n",
    "    else:\n",
    "        return np.log(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用viterbi算法计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, pi, A, B):\n",
    "    x = [word2id[word] for word in x.split(\" \")]#对输入的句子进行切分\n",
    "    T = len(x)\n",
    "    \n",
    "    dp = np.zeros((T,N))\n",
    "    for j in range(N):\n",
    "        dp[0][j] = log(pi[j])+log(A[j][x[0]])# 矩阵第一列初始化\n",
    "    ptr = np.array([[0 for x in range(N)] for y in range(T)])\n",
    "    for i in range(1, T): # 每个单词\n",
    "        for j in range(N): # 每个词性\n",
    "            dp[i][j] = -99999\n",
    "            for k in range(N): # 从每个k到达j\n",
    "                score = dp[i-1][k] + log(B[k][j]) + log(A[j][x[i]])\n",
    "                if score > dp[i][j]:\n",
    "                    dp[i][j] = score\n",
    "                    ptr[i][j] = k\n",
    "    # decoding:把做好的路径打印出来\n",
    "    best_seq = [0]*T\n",
    "    # step1: 找出对应于最后一个单词的词性\n",
    "    best_seq[T-1] = np.argmax(dp[T-1])\n",
    "    for i in range(T-2, -1, -1):\n",
    "        best_seq[i] = ptr[i+1][best_seq[i+1]]\n",
    "    # 将词性序列打印出来\n",
    "    for i in range(len(best_seq)):\n",
    "        print(id2tag[best_seq[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT\n",
      "NN\n",
      "NN\n",
      ",\n",
      "DT\n",
      "NN\n",
      "CC\n",
      "NNS\n",
      "IN\n",
      "DT\n",
      "NNS\n",
      "VBN\n",
      "IN\n",
      "DT\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "x = \"Social security number , passport number and details about the services provided for the payment\"\n",
    "viterbi(x,pi,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
