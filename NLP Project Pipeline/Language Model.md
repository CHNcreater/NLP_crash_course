# Language Model 语言模型

## 前言

本小节内容主要介绍语言模型的相关知识。

## 1 Noise Channel Model 噪声信道模型

是一个普适性的建模模型，被广泛应用于语音识别、拼写纠错、机器翻译、中文分词、词性标注、音字转换等众多领域。

用公式来表示就是，
$$
p(text|source)\propto p(source|text)p(text)
$$
$p(text)$这部分的概率就是通过语言模型得到的。

例如，机器翻译
$$
p(中文|英文)\propto p(英文|中文)p(中文)
$$
其中，p(中文)由语言模型得到。

## 2 语言模型

用来判断一句话是否在语法上通顺。语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为$T$的文本中的词依次为$w_1,w_2,...,w_T$，那么在离散的时间序列中，$w_t（1≤t≤T）$可看作在时间步（time step）tt的输出或标签。给定一个长度为$T$的词的序列$w_1,w_2,...,w_T$，语言模型将计算该序列的概率：
$$
p(w_1,w_2,...,w_T)
$$
语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。

### 2.1 Chain Rule 链式法则

链式法则如下公式所示
$$
p(A,B,C,D)=p(A)p(B|A)p(C|A,B)p(D|A,B,C)
$$
上面公式的前提是不认为$A,B,C,D$是相互独立的。

通过具体例子来说明，
$$
p(厨房，里面，食用油，用光，了)=p(厨房)p(里面|厨房)p(食用油|厨房，里面)p(用光|厨房，里面，食用油)p(了|厨房，里面，食用油，用光，了)
$$
实际计算的过程中，通过统计的方法来计算概率，通过计数的方法计算概率。但是会遇到稀疏性的问题，即存在某些组合在我们的预料中不存在的情况，导致概率越来越低，甚至为零。

### 2.2 马尔可夫假设（Markov Assumption）与N元文法（N-Gram）

> 当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order nn）。

当n分别等于1，2，3时，分别将其称作一元语法（unigram），二元语法（bigram）和三元语法（trigram）。

通过实例来解释这几种语法，

* 一元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2)...p(w_t)
  $$

* 二元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2|w_1)p(w_3|w_2)...p(w_t|w_{t-1})
  $$

* 三元语法
  $$
  p(w_1,w_2,...,w_t)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)...p(w_t|w_{t-1},w_{t-2})
  $$

## 3 基于语言模型计算文本的概率

根据不同的文法规定，采用不同的计算公式计算出对应的概率，统计的方法来计算概率就是通过计数的方式，上文提到了，如果有些单词或者单词序列在语料中不存在，那么概率会变成零，需要采用一些平滑策略来解决该问题。

![](D:\yinbo.qiao\Study Folder\github\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-15_16-50-51.jpg)

## 4 评估语言模型

问题：如何评估训练出来的两个语言模型的好坏？

前提：

* 假设有两个语言模型A，B
* 选定一个特定的任务比如拼写纠错
* 把两个模型都应用于该任务中
* 最后比较准确率，从而判断A，B的表现

但是上面的方法效率太低，时间耗费太长，还要指定相应的下游任务，这样的方法并不理想。于是我们找到另外一种方式来进行评估。

核心思路就是，将训练好的语言模型做一个生成任务，比如完形填空的方式，给出一个单词，来生成下一个单词。

今天____

今天天气____

......

采用的评估准则就是Perplexity困惑度。
$$
Perplexity = 2^{-(x)}\ \ \ \ x:average log likelihood
$$
通过一个例子来说明，如何计算困惑度，

![](D:\yinbo.qiao\Study Folder\github\NLP_crash_course\NLP Project Pipeline\Language Model.assets\2021-06-15_20-25-29.jpg)

一般情况下，Trigram的训练效果是最好的，如果n更大的话，可能会过拟合，另一方面也会提升算法的时间复杂度。

## 5 语言模型的平滑方法

我们观察第三小节中的语料库，如果当前有两句话，

> 句子1：我今天健身没有
>
> 句子2：我今天没有健身

由于语料库中不存在`没有`这个单词，于是两句话计算出来的概率都是0，但是从人类的感知，明显第一个句子要更加的符合语法，也更流畅，但是由于语料库的限制导致了无法区分两个句子的合理性，为了解决这一点，使用平滑方法（Smoothing）。



